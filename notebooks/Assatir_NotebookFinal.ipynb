{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655908a7",
   "metadata": {},
   "source": [
    "# 1. Introduction & Contexte\n",
    "\n",
    "Depuis 1982, la société **Infologic** conçoit et intègre des solutions logicielles dédiées à l’agroalimentaire.  \n",
    "L’évolution et la maintenance de ces logiciels reposent notamment sur des **tests continus** réalisés par des équipes internes.  \n",
    "Chaque testeur utilise différents profils afin de vérifier le bon fonctionnement de multiples fonctionnalités du logiciel.\n",
    "\n",
    "Actuellement, lorsqu’un problème est détecté, les testeurs doivent reporter leurs actions sous leur vrai profil, ce qui demande des manipulations longues et fastidieuses.  \n",
    "\n",
    "**Objectif du projet :**  \n",
    "Étudier la possibilité d’**identifier automatiquement l’utilisateur** d’un logiciel à partir de ses **traces d’utilisation** (séquences d’actions effectuées).  \n",
    "Pour cela, nous allons construire un **modèle de classification** basé sur des techniques de **Machine Learning**.\n",
    "\n",
    "Un tel modèle pourrait :\n",
    "- faciliter le suivi des tests et des utilisateurs internes ;\n",
    "- contribuer à la **détection d’usurpations** ou d’**intrusions logicielles**.\n",
    "\n",
    "**Métrique d’évaluation : F1-score moyen**  \n",
    "Le F1-score mesure l’équilibre entre la **précision** et le **rappel** :\n",
    "$$\n",
    "F1 = 2 \\times \\frac{P \\times R}{P + R}\n",
    "$$\n",
    "Un bon modèle cherchera à maximiser simultanément ces deux aspects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76da341",
   "metadata": {},
   "source": [
    "### *Import des bibliotheques nessecaires*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, make_scorer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67af949",
   "metadata": {},
   "source": [
    "### *Définition des variables, fonctions et classes utiles pour la suite*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235de149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expression régulière principale pour parser les actions utilisateur\n",
    "ACTION_RE = re.compile(\n",
    "    r\"^(?P<base>[^(<$1]+?)\"              # action de base, ex: \"Création d'un écran\"\n",
    "    r\"(?:\\((?P<ctrl>[^)]*)\\))?\"          # (controller/écran)\n",
    "    r\"(?:<(?P<conf>[^>]+)>)?\"            # <configuration>\n",
    "    r\"(?:\\$(?P<chain>[^$]+)\\$)?\"         # $chaine$\n",
    "    r\"(?P<edit>1)?$\"                     # flag édition \"1\"\n",
    ")\n",
    "\n",
    "def read_ds(ds_name: str):\n",
    "    \"\"\"Lit un fichier CSV d’actions utilisateur et définit dynamiquement les colonnes.\"\"\"\n",
    "    with open(f'data/{ds_name}.csv') as f:\n",
    "        max_actions = max((len(str(c).split(\",\")) for c in f.readlines()))\n",
    "        f.seek(0)\n",
    "        _names = [\"util\", \"navigateur\"] if \"train\" in ds_name else [\"navigateur\"]\n",
    "        _names.extend(range(max_actions - len(_names)))\n",
    "        return pd.read_csv(f, names=_names, dtype=str)\n",
    "\n",
    "def row_to_sequence(row, start_col=2):\n",
    "    \"\"\"Transforme une ligne du dataset en séquence d’actions.\"\"\"\n",
    "    vals = []\n",
    "    for c in row.index[start_col:]:\n",
    "        v = row[c]\n",
    "        if pd.isna(v): break\n",
    "        vals.append(str(v))\n",
    "    return vals\n",
    "\n",
    "def normalize_token(tok: str) -> list:\n",
    "    \"\"\"Nettoie et segmente un token brut en sous-tokens normalisés.\"\"\"\n",
    "    tok = tok.strip()\n",
    "    if not tok: return []\n",
    "    if tok.startswith(\"t\") and tok[1:].isdigit():\n",
    "        return [f\"TWIN_{tok[1:]}\"]\n",
    "\n",
    "    m = ACTION_RE.match(tok)\n",
    "    if not m:\n",
    "        return [tok.replace(\" \", \"_\")]\n",
    "    base = m.group(\"base\").strip().replace(\" \", \"_\")\n",
    "    ctrl = (m.group(\"ctrl\") or \"\").strip().replace(\".\", \"_\").replace(\" \", \"_\")\n",
    "    conf = (m.group(\"conf\") or \"\").strip().replace(\" \", \"_\")\n",
    "    chain = (m.group(\"chain\") or \"\").strip().replace(\" \", \"_\")\n",
    "    edit = m.group(\"edit\")\n",
    "\n",
    "    out = [f\"A_{base}\"]\n",
    "    if ctrl:  out.append(f\"C_{ctrl}\")\n",
    "    if conf:  out.append(f\"CFG_{conf}\")\n",
    "    if chain: out.append(f\"CH_{chain}\")\n",
    "    if edit:  out.append(\"EDIT_1\")\n",
    "    return out\n",
    "\n",
    "def seq_to_text(seq_list):\n",
    "    \"\"\"Transforme une séquence de tokens en texte prêt pour la vectorisation TF-IDF.\"\"\"\n",
    "    toks = []\n",
    "    for tok in seq_list:\n",
    "        toks.extend(normalize_token(tok))\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def extract_temporal_features(actions):\n",
    "    \"\"\"\n",
    "    Extrait des caractéristiques temporelles à partir d'une séquence d'actions.\n",
    "    Les marqueurs temporels commencent par 't' et représentent des intervalles de 5s.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Identifier les marqueurs temporels\n",
    "    temporal_markers = [a for a in actions if isinstance(a, str) and a.startswith('t')]\n",
    "    time_values = [int(a[1:]) for a in temporal_markers if a[1:].isdigit()]\n",
    "\n",
    "    # Durée de session et nombre de fenêtres temporelles\n",
    "    features['session_duration'] = max(time_values) if time_values else 0\n",
    "    features['num_time_windows'] = len(temporal_markers)\n",
    "\n",
    "    # Compter les actions entre deux marqueurs temporels\n",
    "    actions_per_window = []\n",
    "    current_window_actions = 0\n",
    "\n",
    "    for action in actions:\n",
    "        if not isinstance(action, str):\n",
    "            continue\n",
    "        if action.startswith('t'):\n",
    "            if current_window_actions > 0:\n",
    "                actions_per_window.append(current_window_actions)\n",
    "            current_window_actions = 0\n",
    "        else:\n",
    "            current_window_actions += 1\n",
    "\n",
    "    if current_window_actions > 0:\n",
    "        actions_per_window.append(current_window_actions)\n",
    "\n",
    "    # Calculer les statistiques de rythme\n",
    "    if actions_per_window:\n",
    "        features['mean_actions_per_window'] = np.mean(actions_per_window)\n",
    "        features['std_actions_per_window'] = np.std(actions_per_window)\n",
    "        features['max_actions_per_window'] = np.max(actions_per_window)\n",
    "        features['min_actions_per_window'] = np.min(actions_per_window)\n",
    "        features['median_actions_per_window'] = np.median(actions_per_window)\n",
    "\n",
    "        total_actions = len([a for a in actions if isinstance(a, str) and not a.startswith('t')])\n",
    "        features['actions_per_second'] = (\n",
    "            total_actions / features['session_duration'] if features['session_duration'] > 0 else 0\n",
    "        )\n",
    "        features['rhythm_stability'] = (\n",
    "            np.std(actions_per_window) / np.mean(actions_per_window) if np.mean(actions_per_window) > 0 else 0\n",
    "        )\n",
    "        features['rhythm_trend'] = (\n",
    "            np.polyfit(range(len(actions_per_window)), actions_per_window, 1)[0]\n",
    "            if len(actions_per_window) > 1\n",
    "            else 0\n",
    "        )\n",
    "    else:\n",
    "        # Valeurs par défaut\n",
    "        for k in [\n",
    "            'mean_actions_per_window', 'std_actions_per_window', 'max_actions_per_window',\n",
    "            'min_actions_per_window', 'median_actions_per_window', 'actions_per_second',\n",
    "            'rhythm_stability', 'rhythm_trend'\n",
    "        ]:\n",
    "            features[k] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "class SelectCols(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Sélectionne un sous-ensemble de colonnes d’un DataFrame.\"\"\"\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        return X[self.cols]\n",
    "\n",
    "class SessionStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extrait des features statistiques à partir d’une séquence d’actions.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        seqs = X[\"seq_raw\"]\n",
    "        feats = []\n",
    "        for seq in seqs:\n",
    "            if not isinstance(seq, list) or len(seq) == 0:\n",
    "                feats.append([0]*10)\n",
    "                continue\n",
    "\n",
    "            n_actions = sum(1 for s in seq if not (isinstance(s, str) and s.startswith(\"t\") and s[1:].isdigit()))\n",
    "            n_twins = sum(1 for s in seq if isinstance(s, str) and s.startswith(\"t\") and s[1:].isdigit())\n",
    "            unique_ctrl = len(set(re.findall(r\"\\((.*?)\\)\", \" \".join(seq))))\n",
    "            unique_actions = len(set(seq))\n",
    "            ratio_unique = unique_actions / (len(seq) or 1)\n",
    "            total_len = len(seq)\n",
    "            ratio_twins = n_twins / (total_len or 1)\n",
    "            ratio_actions = n_actions / (total_len or 1)\n",
    "            n_conf = sum(1 for s in seq if \"CFG_\" in s)\n",
    "            n_chain = sum(1 for s in seq if \"CHAIN_\" in s)\n",
    "            twin_ids = [int(s[1:]) for s in seq if isinstance(s, str) and s.startswith(\"t\") and s[1:].isdigit()]\n",
    "            if twin_ids:\n",
    "                twin_span = max(twin_ids) - min(twin_ids)\n",
    "                twin_gap_mean = np.mean(np.diff(sorted(twin_ids))) if len(twin_ids) > 1 else 0\n",
    "            else:\n",
    "                twin_span = 0\n",
    "                twin_gap_mean = 0\n",
    "\n",
    "            feats.append([\n",
    "                n_actions, n_twins, unique_ctrl, unique_actions, ratio_unique,\n",
    "                ratio_twins, ratio_actions, n_conf, n_chain, twin_span, twin_gap_mean\n",
    "            ])\n",
    "        return np.array(feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8c991",
   "metadata": {},
   "source": [
    "# 2. Exploration des Données (EDA)\n",
    "\n",
    "Avant toute modélisation, il est essentiel de **comprendre les données** disponibles.  \n",
    "Cette étape vise à :\n",
    "- examiner la structure du jeu de données ;\n",
    "- identifier les types de variables (catégorielles, numériques, temporelles) ;\n",
    "- observer la distribution des classes (équilibrée ou non) ;\n",
    "- mettre en évidence des **tendances comportementales** entre utilisateurs.\n",
    "\n",
    "Nous examinerons :\n",
    "- Le dénombrement des actions primaires \n",
    "- Quelques caractéristiques temporelles par trace (statistiques de rythme);\n",
    "- La distribution du nombre de sessions par utilisateur\n",
    "- La distribution de nombre d'actions par trace ;\n",
    "- La durée et rythme d'utilisation par utilisateur\n",
    "\n",
    "- La rareté de certaines actions ;\n",
    "- Le TF-IDF des actions par utilisateur ;\n",
    "- La correlation entre les actions primaires ;\n",
    "\n",
    "Des visualisations permettront de mieux **comprendre le comportement global** des utilisateurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d89be",
   "metadata": {},
   "source": [
    "## 2-1) Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_ds(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739a486",
   "metadata": {},
   "source": [
    "## 2-2) Informations générales sur le dataset + aperçu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872bc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensions du dataset : {df.shape[0]} lignes × {df.shape[1]} colonnes\")\n",
    "print(\"\\nInfos sur les colonnes :\")\n",
    "df.info()\n",
    " \n",
    "# Liste des colonnes d’actions (toutes sauf 'util' et 'navigateur')\n",
    "action_cols = [c for c in df.columns if c not in ['util', 'navigateur']]\n",
    "print(f\"\\nNombre total de colonnes d'actions : {len(action_cols)}\")\n",
    "print(\"\\nAperçu :\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43744feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'utilisateurs uniques\n",
    "num_unique_users = df['util'].nunique()\n",
    "print(f\"Nombre d'utilisateurs uniques : {num_unique_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe28f45",
   "metadata": {},
   "source": [
    "#### Description du jeu de données\n",
    "\n",
    "- **Nombre de lignes (traces)** : 3 279  \n",
    "- **Nombre de colonnes** : 14 470  \n",
    "- **Nombre d'utlisateurs (classes)**: 247\n",
    "\n",
    "#### Structure des colonnes\n",
    "\n",
    "1. **Colonne `utilisateur`** : identifiant ou nom de l’utilisateur.  \n",
    "2. **Colonne `navigateur`** : navigateur utilisé (ex. Chrome, Firefox, Safari, etc.).  \n",
    "3. **Le reste des colonnes** correspond à la **série des actions** réalisées par l’utilisateur sur le site pendant sa visite.  \n",
    "   - Chaque action est associée à un **indicateur temporel** `tXX`, incrémenté de **5 secondes** entre chaque action réalisée.\n",
    "\n",
    "---\n",
    "\n",
    "#### Exemple de lecture d’une ligne\n",
    "\n",
    "Une ligne du jeu de données se lit de la manière suivante :\n",
    "utilisateur + navigateur + action1 + t5 + action2 +action3 + t10 +...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db85e2",
   "metadata": {},
   "source": [
    "## 2-3) Dénombrement des actions primaires\n",
    "Les actions primaires correspondent à la premiere section de l'action qui précéde l'un des caracteres suivants: (, <, $ ou [\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes d’action (toutes sauf 'util' et 'navigateur')\n",
    "action_cols = [col for col in df.columns if col not in ['util', 'navigateur']]\n",
    "actions = df[action_cols].values.flatten()\n",
    "actions = [str(a).strip() for a in actions if pd.notna(a) and not str(a).startswith('t') and not re.fullmatch(r'\\d+', str(a)) and str(a).lower() != 'none']\n",
    "actions_uniques = sorted(set(actions))\n",
    "# Nombre d'actions uniques\n",
    "print('Nombre d\\'actions uniques:', len(actions_uniques))\n",
    "actions_uniques\n",
    "# Nettoyer chaque action\n",
    "def nettoyer_action(x):\n",
    "    txt = str(x).strip()\n",
    "    # Garder uniquement le texte avant la parenthèse\n",
    "    txt = re.split(r'[\\(<\\$\\[]', txt)[0].strip()\n",
    "    if txt.endswith('1') and len(txt) > 1:\n",
    "        txt = txt[:-1].strip()\n",
    "    # Retourner None si le texte est vide après nettoyage     \n",
    "    return txt\n",
    "\n",
    "actions_nettoyees = pd.Series(actions_uniques).apply(nettoyer_action).dropna()\n",
    "\n",
    "# Supprimer les doublons et trier\n",
    "actions_primaires_uniques = (\n",
    "    actions_nettoyees.drop_duplicates()\n",
    "    .sort_values()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Liste finale des actions uniques\n",
    "print(f\"\\nNombre d'actions uniques trouvées : {len(actions_primaires_uniques)}\")\n",
    "print(actions_primaires_uniques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5200c",
   "metadata": {},
   "source": [
    "## 2-4) Extraction des caractéristiques temporelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : appliquer la fonction sur un sous-échantillon\n",
    "temporal_features_df = df.head(100).apply(extract_temporal_features)\n",
    "temporal_features_df = pd.DataFrame(temporal_features_df.tolist())\n",
    "\n",
    "print(\"Aperçu des features temporelles :\")\n",
    "\n",
    "# Fusion avec les métadonnées utilisateur/navigateur\n",
    "df_features = pd.concat([df[['util', 'navigateur']].head(100).reset_index(drop=True),\n",
    "                         temporal_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "display(df_features.head())\n",
    "#voir session duration=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072ec48",
   "metadata": {},
   "source": [
    "## 2-5) Visualistation des données:\n",
    "### i) Courbe de densité du nombre de sessions par utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ca5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sessions par utilisateur\n",
    "sessions_per_user = df['util'].value_counts()\n",
    "# Affichage de la distribution du nombre de sessions par utilisateur\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(sessions_per_user, bins=30, kde=True)\n",
    "plt.title(\"Distribution du nombre de sessions par utilisateur\")\n",
    "plt.xlabel(\"Nombre de sessions\")\n",
    "plt.ylabel(\"Nombre d'utilisateurs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec791a",
   "metadata": {},
   "source": [
    "### ii) Courbe de densité du nombre d'actions par trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_actions = [col for col in df.columns if col not in ['util', 'navigateur']]\n",
    "\n",
    "# Calcul du nombre d'actions (valeurs non nulles) par utilisateur / trace\n",
    "df['nb_actions'] = df[colonnes_actions].notna().sum(axis=1)-2  # -2 pour exclure 'util' et 'navigateur'\n",
    "\n",
    "# --- Étape 3 : calcul du 90e percentile ---\n",
    "p90 = np.percentile(df['nb_actions'], 90)\n",
    "\n",
    "# --- Étape 4 : histogramme + densité ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['nb_actions'], bins=80, density=True, alpha=0.4, edgecolor='black', label=\"Histogramme\")\n",
    "\n",
    "x_vals = np.linspace(df['nb_actions'].min(), df['nb_actions'].max(), 500)\n",
    "kde = gaussian_kde(df['nb_actions'])\n",
    "plt.plot(x_vals, kde(x_vals), color='orange', linewidth=2, label=\"Courbe de densité (KDE)\")\n",
    "\n",
    "# --- Étape 5 : ligne verticale à 90% ---\n",
    "plt.axvline(p90, color='green', linestyle='--', linewidth=2, label=f\"90ᵉ percentile = {p90:.1f} actions\")\n",
    "\n",
    "# --- Étape 6 : annotation du seuil ---\n",
    "plt.text(p90 + (df['nb_actions'].max()*0.01), max(kde(x_vals))*0.9,\n",
    "         f\"90% des traces ≤ {p90:.1f} actions\",\n",
    "         rotation=90, color='green', va='top', fontsize=10)\n",
    "\n",
    "# --- Étape 7 : mise en forme ---\n",
    "plt.title(\"Distribution du nombre d'actions par trace\", fontsize=14)\n",
    "plt.xlabel(\"Nombre d'actions\", fontsize=12)\n",
    "plt.ylabel(\"Densité\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, df['nb_actions'].max()+5, step=max(1, df['nb_actions'].max()//20)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2f0c3",
   "metadata": {},
   "source": [
    "La courbe de densité  ci-dessus montre que pour plus de 90% des traces on ne dépasse pas 2075 actions.\n",
    "En supprimant les autres traces, on peut supprimer les colonnes vides mais cela exculu un utilisateur {'fxg'} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage des traces avec moins de 2078 actions car il s'agit du 90e percentile\n",
    "df_clean9 = df[df['nb_actions'] <= p90].drop(columns=['nb_actions'])\n",
    "#suppression des colonnes entièrement vides\n",
    "df_clean9 = df_clean9.dropna(axis=1, how='all')\n",
    "exclusion_utilisateur = set(df['util']) - set(df_clean9['util'])\n",
    "exclusion_utilisateur\n",
    "print('Utilisateur exclu:', exclusion_utilisateur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dacd25",
   "metadata": {},
   "source": [
    "### iii) Durée et rythme d’utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905323f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : durée et rythme d’utilisation\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x='util', y='session_duration', data=df_features)\n",
    "plt.title(\"Durée moyenne des sessions par utilisateur\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x='util', y='actions_per_second', data=df_features)\n",
    "plt.title(\"Rythme d’actions par seconde selon l’utilisateur\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89920da7",
   "metadata": {},
   "source": [
    "En général, les utilisateurs ont un comportement de navigation récurrent : ils utilisent le site de manière similaire à chaque visite, avec une durée et une vitesse d’interaction comparables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b8929",
   "metadata": {},
   "source": [
    "### iv) Fréquence des actions par utilisateur (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nettoyer_actions(trace: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = trace.copy()\n",
    "    # Identifier les colonnes d'actions\n",
    "    colonnes_actions = [c for c in df.columns if c not in ['util', 'navigateur']]\n",
    "    # Application du nettoyage à toutes les colonnes d'action\n",
    "    df[colonnes_actions] = df[colonnes_actions].applymap(nettoyer_action)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "df_clean=nettoyer_actions(df_clean)\n",
    "# Transformation en format long\n",
    "df_long = df_clean.melt(id_vars=['util'], value_vars=colonnes_actions, value_name='action')\n",
    "df_long = df_long.dropna(subset=['action'])\n",
    "# Filtrer uniquement les actions primaires\n",
    "df_long = df_long[df_long['action'].isin(actions_primaires_uniques)]\n",
    "# Compter les occurrences par utilisateur et action\n",
    "freq = df_long.groupby(['util', 'action']).size().unstack(fill_value=0)\n",
    "# Calcul du nombre total d’actions par utilisateur (Lj)\n",
    "total_actions = freq.sum(axis=1)\n",
    "# Calcul TF (Freqi,j / Lj)\n",
    "tf = freq.div(total_actions, axis=0)\n",
    "#TF final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes d’actions\n",
    "cols_actions = [c for c in df_clean.columns if c not in ['util', 'navigateur']]\n",
    "\n",
    "# Mise au format long\n",
    "df_long = df_clean.melt(id_vars='util', value_vars=cols_actions,\n",
    "                        var_name='step', value_name='action')\n",
    "\n",
    "# Supprimer les indicateurs temporels \n",
    "df_long = df_long[~df_long['action'].astype(str).str.match(r'^t\\d+$')]\n",
    "\n",
    "# Compter le nombre d’actions distinctes par utilisateur\n",
    "count_actions = (\n",
    "    df_long.groupby('util')['action']\n",
    "    .nunique()\n",
    "    .reset_index(name='nb_actions_distinctes')\n",
    ")\n",
    "\n",
    "# Visualiser les résultats\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(count_actions['util'], count_actions['nb_actions_distinctes'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Nombre d’actions primaires distinctes par utilisateur\")\n",
    "plt.xlabel(\"Utilisateur\")\n",
    "plt.ylabel(\"Nombre d’actions primaires distinctes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53307d",
   "metadata": {},
   "source": [
    "L'analyse initiale du nombre d'actions primaires distinctes par utilisateur révèle une hétérogénéité comportementale significative.  \n",
    "\n",
    "Le graphique montre que :\n",
    "- La **majorité des utilisateurs** effectue entre **30 et 50 actions distinctes**, constituant un comportement de base\n",
    "- Certains utilisateurs se démarquent avec des **pics notables** (jusqu'à **~97 actions distinctes**), indiquant des profils d'usage plus diversifiés ou intensifs\n",
    "- Cette **variabilité dans la diversité d'actions** suggère que chaque utilisateur possède une **signature comportementale propre**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682ee31",
   "metadata": {},
   "source": [
    "##### -> Heatmap de la fréquence des actions chez les utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66968926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par exemple, on prend les 20 actions les plus fréquentes globalement\n",
    "actions_top = tf.sum(axis=0).sort_values(ascending=False).head(20).index\n",
    "tf_top = tf[actions_top]\n",
    "\n",
    "# Création de la heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(tf_top, annot=False, cmap=\"YlGnBu\", linewidths=0.5)\n",
    "plt.title(\"TF (Term Frequency) des actions par utilisateur\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Utilisateur\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4470b",
   "metadata": {},
   "source": [
    "Le visuel représente une **matrice** où :  \n",
    "- les **lignes** correspondent aux **utilisateurs**,  \n",
    "- les **colonnes** aux **actions primaires** réalisées,  \n",
    "- et les **cellules** indiquent la **fréquence** d’apparition de chaque action chez un utilisateur.  \n",
    "\n",
    "On observe que **certaines actions** — notamment l’**exécution d’un bouton** — apparaissent fréquemment chez la majorité des utilisateurs.  \n",
    "Ces actions communes ne sont donc **pas déterminantes** pour distinguer un utilisateur d’un autre.  \n",
    "\n",
    "En revanche, les **actions situées à droite de la matrice** sont beaucoup plus **spécifiques**, n’apparaissant que chez un petit nombre d’utilisateurs.  \n",
    "\n",
    "Ainsi, la **prise en compte du TF (Term Frequency)** devient intéressante pour le **modèle de classification**, car elle permet de quantifier la fréquence et l’importance relative de chaque action dans le profil utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4acc97",
   "metadata": {},
   "source": [
    "### v) Visualisation du IDF des actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c37abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf est ton DataFrame TF (util x actions)\n",
    "# freq est le DataFrame des occurrences (util x actions)\n",
    "\n",
    "# Nombre total d'utilisateurs (documents)\n",
    "N = freq.shape[0]\n",
    "# Nombre d'utilisateurs contenant chaque action\n",
    "ni = (freq > 0).sum(axis=0)\n",
    "# Calcul de l'IDF selon la formule classique\n",
    "idf = np.log(N / (1 + ni))\n",
    "# Conversion en DataFrame pour facilité\n",
    "idf_df = pd.DataFrame(idf, columns=['IDF'])\n",
    "#print(idf_df.sort_values(by='IDF', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75731d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier les actions par IDF décroissante et prendre les 20 plus rares\n",
    "actions_rare = idf_df['IDF'].sort_values(ascending=False).head(20).index\n",
    "idf_top = idf_df.loc[actions_rare]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=idf_top.index, y=idf_top['IDF'], palette=\"YlGnBu\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Top 20 des actions les plus rares (IDF)\")\n",
    "plt.ylabel(\"IDF\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Le graphique à barres affiche par ordre décroissant la rareté des actions primaires dans le jeu de données.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba97bfb",
   "metadata": {},
   "source": [
    "### vi) Visualisation de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf * idf\n",
    "actions_top = tfidf.sum(axis=0).sort_values(ascending=False).head(20).index\n",
    "tfidf_top = tfidf[actions_top]\n",
    "\n",
    "# Création de la heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(tfidf_top, annot=False, cmap=\"YlGnBu\", linewidths=0.5)\n",
    "plt.title(\"TF-IDF des actions par utilisateur\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Utilisateur\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad94e1f",
   "metadata": {},
   "source": [
    "## Justification du choix du TF-IDF\n",
    "\n",
    "L'hétérogénéité observée dans la distribution des actions justifie l'utilisation du **TF-IDF** car cette méthode permet de valoriser les actions rares en attribuant un poids élevé aux comportements distinctifs grâce à la composante IDF, tout en réduisant le poids des actions communes qui risqueraient de dominer la représentation et de masquer les différences réelles entre utilisateurs. La pondération TF × IDF équilibre ainsi la fréquence et la spécificité de chaque action, reflétant leur pertinence discriminante et créant des features exploitables qui facilitent la séparation des clusters en classification. Ainsi, le **TF-IDF** convertit efficacement la diversité comportementale observée en **représentations vectorielles discriminantes** pour la classification des utilisateurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb64c3",
   "metadata": {},
   "source": [
    "## 2-6) Etude correlation entre les actions primaires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5992ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on souhaite voir si des actions sont correles entre elles \n",
    "action_corr = tfidf.corr()\n",
    "#visualisation de la matrice de corrélation\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(action_corr, annot=False, cmap=\"coolwarm\", center=0, linewidths=0.5)\n",
    "plt.title(\"Matrice de corrélation des actions (TF-IDF)\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Action\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d80517",
   "metadata": {},
   "source": [
    "La matrice de correlation indique une forte correlation entre quelques actions et leurs opposées :\n",
    "- Sélection d'un élément/ Désélection d'un élément \n",
    "- Affichage d'une dialogue/ Fermeture d'une dialogue\n",
    "- Ouverture d'un panel / Fermeture d'un panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73c818",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7068adc",
   "metadata": {},
   "source": [
    "Dans cette section, nous préparons les données en combinant trois types de features complémentaires :  \n",
    "\n",
    "- **Séquences d’actions** : normalisées et vectorisées en texte via TF-IDF pour capturer la sémantique des interactions.  \n",
    "- **Métadonnées techniques** : encodées en one-hot pour exploiter des informations comme le navigateur utilisé.  \n",
    "- **Statistiques de session** : dérivées des séquences (ex. nombre d’actions, de fenêtres temporelles, etc.) pour résumer le comportement global.\n",
    "\n",
    "Ces différentes vues sont intégrées dans un pipeline unique via `FeatureUnion`, ce qui garantit une gestion cohérente entre entraînement et inférence.\n",
    "\n",
    "\n",
    "####  Objectif et Stratégie de Modélisation\n",
    "\n",
    "Nous cherchons à construire un modèle capable de classifier des comportements utilisateurs à partir de données très variées(numériques et textuelles). Concrètement, nous avons accès à des séquences d’actions, à des informations techniques sur le navigateur utilisé, ainsi qu’à des métriques synthétisant l’activité de chaque session. Pour exploiter ces sources complémentaires de manière cohérente, il est essentiel de les intégrer dans un cadre commun.\n",
    "Les séquences d'actions, très riches, capturent à la fois l'ordre et la nature des interactions réalisées. Les données sur le navigateur, elles, apportent des informations catégorielles utiles au contexte. Enfin, les indicateurs agrégés fournissent un résumé global de l’activité. Pour combiner efficacement ces différents types de données, nous utilisons une architecture en Feature Union : elle nous permet de créer des traitements adaptés pour chaque modalité tout en les rassemblant dans un pipeline unique, utilisable aussi bien à l’entraînement qu'à l’inférence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e502bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "train = read_ds(\"train\")\n",
    "test = read_ds(\"test\")\n",
    "\n",
    "print(f\" Dimensions originales:\")\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "\n",
    "# Vérifier la structure des données\n",
    "print(f\"\\n Structure des données:\")\n",
    "print(f\"Type train: {type(train)}\")\n",
    "print(f\"Type test: {type(test)}\")\n",
    "\n",
    "if hasattr(train, 'columns'):\n",
    "    print(f\"Colonnes train: {train.columns.tolist()[:5] if len(train.columns) > 0 else 'AUCUNE'}\")\n",
    "else:\n",
    "    print(\"Train n'a pas de noms de colonnes\")\n",
    "\n",
    "# Aperçu des premières valeurs\n",
    "print(f\"\\n Aperçu des données (premières lignes):\")\n",
    "try:\n",
    "    print(train.iloc[:2, :3])\n",
    "except:\n",
    "    print(\"Accès par iloc impossible - structure particulière\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f3d4f",
   "metadata": {},
   "source": [
    "**La structure des données a été analysée avec succès.** Nous disposons de :\n",
    "- **3,279 sessions** d'entraînement avec 14,473 colonnes\n",
    "- **324 sessions** de test avec 7,729 colonnes  \n",
    "- **Colonnes d'actions** : `action_0` à `action_14469` pour le train, `action_0` à `action_7725` pour le test\n",
    "- **La colonne `action_1`** a été identifiée comme contenant les informations navigateur\n",
    "\n",
    "Cette structure confirme la nature séquentielle des données, où chaque colonne représente une action chronologique dans la session utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f7d56",
   "metadata": {},
   "source": [
    "## 3.1. Extraction des Séquences d'Actions : Transformation Structurelle\n",
    "\n",
    "Initialement, les données sont stockées dans un format tabulaire \"wide\" où les actions successives sont réparties sur plusieurs colonnes. Cependant, cette représentation s'avère sous-optimale pour l'analyse comportementale car elle ne respecte pas la nature séquentielle des données.\n",
    "\n",
    "Par conséquent, nous procédons à une transformation fondamentale : convertir cette structure bidimensionnelle en séquences temporelles unidimensionnelles. Concrètement, cette opération consiste à parcourir chaque ligne horizontalement jusqu'à rencontrer une valeur manquante, ce qui délimite naturellement la fin de la session.\n",
    "\n",
    "En termes de représentation, cette transformation offre plusieurs avantages déterminants. Premièrement, elle préserve l'ordre chronologique des actions, qui est souvent porteur de sens dans l'analyse comportementale. Deuxièmement, elle rend les données compatibles avec les techniques de traitement du langage naturel. Enfin, elle permet une gestion uniforme des sessions de longueurs variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d25511",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"seq_raw\"] = train.apply(lambda r: row_to_sequence(r, start_col=2), axis=1)\n",
    "test[\"seq_raw\"]  = test.apply(lambda r: row_to_sequence(r, start_col=1), axis=1)\n",
    "print(f\"Train - {len(train)} sessions, longueur moyenne: {train['seq_raw'].apply(len).mean():.1f}\")\n",
    "print(f\"Test - {len(test)} sessions, longueur moyenne: {test['seq_raw'].apply(len).mean():.1f}\")\n",
    "print(f\"Exemple session 0: {train['seq_raw'].iloc[0][:3]}...\")  # 3 premières actions\n",
    "print(\" Extraction terminée\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Séquences extraites:\")\n",
    "print(f\"Train - {len(train['seq_raw'])} sessions, longueur moyenne: {train['seq_raw'].apply(len).mean():.1f}\")\n",
    "print(f\"Test - {len(test['seq_raw'])} sessions, longueur moyenne: {test['seq_raw'].apply(len).mean():.1f}\")\n",
    "\n",
    "# Analyse des longueurs\n",
    "train_lengths = train['seq_raw'].apply(len)\n",
    "print(f\"\\n Distribution des longueurs:\")\n",
    "print(f\"  Min: {train_lengths.min()}, Max: {train_lengths.max()}\")\n",
    "print(f\"  Moyenne: {train_lengths.mean():.1f}, Médiane: {train_lengths.median()}\")\n",
    "print(f\"  Sessions > 1000 actions: {(train_lengths > 1000).sum()}\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(train_lengths, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Distribution des longueurs de sessions')\n",
    "plt.xlabel('Nombre d\\'actions')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.axvline(train_lengths.mean(), color='red', linestyle='--', label=f'Moyenne: {train_lengths.mean():.1f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2ef18",
   "metadata": {},
   "source": [
    "## 3.2.  Normalisation Sémantique des Tokens : Décomposition Linguistique\n",
    "\n",
    "À ce stade, nous disposons de séquences brutes qui nécessitent une normalisation approfondie. En effet l'analyse préliminaire révèle que les actions utilisateur suivent des patterns linguistiques structurés mais non standardisés. Plus spécifiquement nous observons systématiquement la présence de composants sémantiques distincts : actions principales, contextes d'exécution, configurations techniques et données métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"seq_txt\"] = train[\"seq_raw\"].apply(seq_to_text)\n",
    "test[\"seq_txt\"]  = test[\"seq_raw\"].apply(seq_to_text)\n",
    "print(\" Test de normalisation sur cas types:\")\n",
    "test_cases = [\n",
    "    \"Création d'un écran(HomeController)<config_menu>$user_data$1\",\n",
    "    \"Modification(UserProfile)\",\n",
    "    \"t25\",\n",
    "    \"Double-clic\"\n",
    "]\n",
    "for case in test_cases:\n",
    "    result = normalize_token(case)\n",
    "    print(f\"  '{case}' → {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification que seq_txt a été créé\n",
    "print(f\"Colonne seq_txt créée: {'seq_txt' in train.columns}\")\n",
    "\n",
    "if 'seq_txt' in train.columns:\n",
    "    # Test sur des exemples types\n",
    "    test_cases = [\n",
    "        \"Création d'un écran(HomeController)<config_menu>$user_data$1\",\n",
    "        \"Modification(UserProfile)\",\n",
    "        \"t25\", \n",
    "        \"Double-clic\",\n",
    "        \"Suppression<confirm_dialog>1\"\n",
    "    ]\n",
    "\n",
    "    print(\" Test de normalisation:\")\n",
    "    for case in test_cases:\n",
    "        result = normalize_token(case)\n",
    "        print(f\"  '{case}'\")\n",
    "        print(f\"  → {result}\")\n",
    "\n",
    "    # Analyse statistique de la normalisation\n",
    "    print(f\"\\n Analyse sur 500 actions aléatoires:\")\n",
    "    sample_actions = []\n",
    "    for session in train['seq_raw'].sample(n=min(500, len(train)), random_state=42):\n",
    "        sample_actions.extend(session[:3])  # 3 actions par session\n",
    "\n",
    "    component_stats = {'base': 0, 'ctrl': 0, 'conf': 0, 'chain': 0, 'edit': 0, 'twin': 0, 'simple': 0}\n",
    "\n",
    "    for action in sample_actions:\n",
    "        if isinstance(action, str):\n",
    "            if action.startswith(\"t\") and action[1:].isdigit():\n",
    "                component_stats['twin'] += 1\n",
    "            else:\n",
    "                normalized = normalize_token(action)\n",
    "                for token in normalized:\n",
    "                    if token.startswith('A_'): component_stats['base'] += 1\n",
    "                    elif token.startswith('C_'): component_stats['ctrl'] += 1\n",
    "                    elif token.startswith('CFG_'): component_stats['conf'] += 1\n",
    "                    elif token.startswith('CH_'): component_stats['chain'] += 1\n",
    "                    elif token == 'EDIT_1': component_stats['edit'] += 1\n",
    "                    else: component_stats['simple'] += 1\n",
    "\n",
    "    print(\"Composants extraits:\")\n",
    "    for comp, count in component_stats.items():\n",
    "        percentage = (count / len(sample_actions)) * 100\n",
    "        print(f\"  {comp}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Vérification de la transformation texte\n",
    "    print(f\"\\nTransformation texte:\")\n",
    "    print(f\"Sessions avec seq_txt: {len(train['seq_txt'])}\")\n",
    "    print(f\"Longueur moyenne seq_txt: {train['seq_txt'].str.len().mean():.1f} caractères\")\n",
    "    print(f\"Exemple: '{train['seq_txt'].iloc[0][:100]}...'\")\n",
    "else:\n",
    "    print(\" ERREUR: La colonne seq_txt n'a pas été créée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0f6a6",
   "metadata": {},
   "source": [
    "La décomposition des actions en composants sémantiques a démontré une performance remarquable avec un taux de structuration de **78.2%** pour les actions de base. L'analyse approfondie de 500 actions aléatoires révèle une richesse informationnelle exceptionnelle :\n",
    "\n",
    "###  Répartition des Composants Extraits\n",
    "- **Actions de base (A_)** : 78.2% - Cœur sémantique des interactions utilisateur\n",
    "- **Contrôleurs/Contextes (C_)** : 46.1% - Information contextuelle très riche  \n",
    "- **Configurations techniques (CFG_)** : 16.0% - Paramètres et configurations système\n",
    "- **Tokens temporels (TWIN_)** : 21.8% - Marqueurs d'interaction temporelle significatifs\n",
    "- **Données métier (CH_)** : 2.6% - Variables et données spécifiques\n",
    "- **Flags d'édition (EDIT_)** : 0.8% - Modes d'interaction avancés\n",
    "\n",
    "### Performance de la Transformation\n",
    "- **Couverture complète** : 100% des sessions transformées (3,279 sessions)\n",
    "- **Densité informationnelle** : Longueur moyenne de 23,810 caractères par session\n",
    "- **Précision de décomposition** : Aucune action non structurée (0% de 'simple')\n",
    "\n",
    "Cette granularité sémantique permet au modèle d'apprendre séparément l'importance de chaque dimension comportementale offrant une base solide pour la classification avancée des patterns utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060126ca",
   "metadata": {},
   "source": [
    "## 3.3. Architecture Pipeline : Design Pattern pour le Feature Engineering\n",
    "\n",
    "Notre pipeline de préparation des données repose sur deux transformers personnalisés : **SelectCols** et **SessionStats**, conçus pour traiter efficacement les séquences d’actions des utilisateurs.\n",
    "\n",
    "### SelectCols : Sélection Intelligente\n",
    "\n",
    "**SelectCols** permet de sélectionner dynamiquement les colonnes pertinentes, offrant flexibilité et réutilisabilité sur différentes sources de données.  \n",
    "Résultat : réduction de trois colonnes initiales à deux colonnes essentielles pour les étapes suivantes.\n",
    "\n",
    "### SessionStats : Analyse Comportementale\n",
    "\n",
    "**SessionStats** transforme les séquences brutes en indicateurs clés :  \n",
    "- **Actions fonctionnelles** : engagement utilisateur  \n",
    "- **Interactions temporelles** : complexité des séquences  \n",
    "- **Diversité des contrôleurs** : exploration de l’application  \n",
    "\n",
    "\n",
    "### Valeur Ajoutée\n",
    "\n",
    "Ces transformers automatisent l’analyse, assurent cohérence et reproductibilité, et restent facilement évolutifs. La validation manuelle confirme la fiabilité des indicateurs pour la modélisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test des transformers\n",
    "test_sample = pd.DataFrame({\n",
    "    'seq_txt': train['seq_txt'].head(3),\n",
    "    'navigateur': train['navigateur'].head(3),\n",
    "    'seq_raw': train['seq_raw'].head(3)\n",
    "})\n",
    "\n",
    "print(\" Test SelectCols:\")\n",
    "selector = SelectCols(['seq_txt', 'navigateur'])\n",
    "result_selector = selector.transform(test_sample)\n",
    "print(f\"  Input: {test_sample.shape}, Output: {result_selector.shape}\")\n",
    "\n",
    "print(\" Test SessionStats:\")\n",
    "stats_calc = SessionStats()\n",
    "result_stats = stats_calc.transform(test_sample)\n",
    "print(f\"  Input: {test_sample.shape}, Output: {result_stats.shape}\")\n",
    "print(f\"  Métriques calculées: {result_stats}\")\n",
    "\n",
    "# Vérification manuelle\n",
    "print(\"\\n Vérification manuelle des calculs:\")\n",
    "for i in range(min(2, len(test_sample))):\n",
    "    session = test_sample['seq_raw'].iloc[i]\n",
    "    if isinstance(session, list):\n",
    "        n_actions = sum(1 for s in session if not (isinstance(s, str) and s.startswith(\"t\") and s[1:].isdigit()))\n",
    "        n_twins = sum(1 for s in session if isinstance(s, str) and s.startswith(\"t\") and s[1:].isdigit())\n",
    "        unique_ctrl = len(set(re.findall(r\"\\((.*?)\\)\", \" \".join(session))))\n",
    "        print(f\"  Session {i}: actions={n_actions}, twins={n_twins}, ctrl={unique_ctrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d137f0c",
   "metadata": {},
   "source": [
    "\n",
    "L'analyse des comportements utilisateurs repose sur 11 métriques comportementales conçues pour offrir une vision riche et complète. Ces métriques capturent non seulement le volume d'utilisation, mais également la diversité des actions réalisées, la dynamique temporelle des usages et l'efficacité des interactions.\n",
    "\n",
    "### Dimensions des Métriques\n",
    "\n",
    "Les 11 métriques sont regroupées en quatre dimensions principales :\n",
    "\n",
    "**Volume**  \n",
    "Inclut le nombre total d'actions (`n_actions`) ainsi que le nombre de fenêtres temporelles actives (`n_twins`). Ces indicateurs traduisent l'intensité générale d'utilisation d'un utilisateur.\n",
    "\n",
    "**Diversité**  \n",
    "Représentée par le ratio d'actions uniques (`ratio_unique`) et le nombre d'actions différentes effectuées (`n_unique_actions`). Ces métriques mesurent la variété des interactions avec le système.\n",
    "\n",
    "**Temporalité**  \n",
    "Inclut l'étendue temporelle de la session (`twin_span`) et le temps moyen entre deux fenêtres d'activité (`twin_gap_mean`). Elles offrent un aperçu du rythme et de la structure de l'utilisation dans le temps.\n",
    "\n",
    "**Efficacité**  \n",
    "Mesurée par le ratio d'actions productives (`ratio_actions`), qui permet d'évaluer la proportion d'actions réellement fonctionnelles ou orientées vers un objectif.\n",
    "\n",
    "### Exemples d'Insights\n",
    "\n",
    "Ces métriques permettent de révéler des comportements d'utilisation qui passeraient inaperçus avec des analyses plus simples.\n",
    "\n",
    "Par exemple, une session caractérisée par 634 actions uniques montre une forte expertise d'utilisation, mais un ratio de diversité de seulement 20% suggère une répétition fréquente des mêmes actions. À l'inverse, une session courte présentant une diversité de 51% montre un utilisateur explorant une grande variété de fonctionnalités dans un laps de temps réduit.\n",
    "\n",
    "Il est également possible de mettre en évidence des profils d'usage équilibrés, combinant à la fois une bonne régularité dans le temps et une variation pertinente des actions effectuées.\n",
    "\n",
    "### Bénéfices pour la Modélisation et l'Analyse\n",
    "\n",
    "L'utilisation combinée de ces 11 fonctionnalités apporte plusieurs avantages :\n",
    "\n",
    "- Une meilleure capacité de discrimination entre des profils d'utilisateurs proches.\n",
    "- Une robustesse accrue grâce à une redondance contrôlée entre certaines métriques.\n",
    "- Une interprétabilité directe, utile pour les équipes produit, métier ou UX.\n",
    "- Une capacité renforcée à identifier des clusters ou des segments d'utilisateurs aux comportements spécifiques.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Cette approche multidimensionnelle, reposant sur des métriques variées, contribue à transformer des logs utilisateurs bruts en insights tangibles et actionnables. Elle constitue une base solide pour des modèles de classification, de segmentation comportementale ou d'amélioration de l'expérience utilisateur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec3963",
   "metadata": {},
   "source": [
    "## 3.4.  Feature Union - **Combinaison Stratégique**\n",
    "\n",
    "Dans notre approche, nous avons voulu traiter chaque type de donnée avec la méthode la plus adaptée, selon la philosophie \"diviser pour mieux régner\".\n",
    "\n",
    "Pour les séquences textuelles, nous avons choisi TF-IDF plutôt que Bag-of-Words ou embeddings, car cela nous permet de pondérer l’importance des termes, de rester léger et interprétable, et d’éviter la malédiction dimensionnelle.\n",
    "\n",
    "Pour la catégorie du navigateur, nous avons utilisé un One-Hot Encoding. Nous avons pensé que ce choix était le plus approprié, car il n’y a pas d’ordre naturel entre les navigateurs et il gère bien les nouvelles catégories.\n",
    "\n",
    "Enfin, nous avons décidé d’ajouter des features statistiques agrégées comme le nombre d’actions ou de doublons. Ces métriques, choisies manuellement, nous permettent d’intégrer notre expertise métier et de compléter les informations textuelles de façon interprétable.\n",
    "\n",
    "Nous avons combiné ces trois pipelines avec un FeatureUnion et utilisé la parallélisation pour accélérer le traitement sur plusieurs cœurs, ce qui nous semblait essentiel pour un travail efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad90bc6",
   "metadata": {},
   "source": [
    "##### Branche TF-IDF (Représentation Sémantique)\n",
    "- **Objectif** : Capturer la sémantique des séquences d'actions\n",
    "- **Technique** : Vectorisation TF-IDF avec paramètres optimisés\n",
    "- **Capacité** : ~20,000 features textuelles\n",
    "- **Avantage** : Pondération intelligente des actions importantes\n",
    "\n",
    "#####  Branche Navigateur (Représentation Contextuelle) \n",
    "- **Objectif** : Modéliser l'influence du contexte technique\n",
    "- **Technique** : One-Hot Encoding robuste\n",
    "- **Capacité** : 1 feature par navigateur unique\n",
    "- **Avantage** : Gestion des nouvelles valeurs en production\n",
    "\n",
    "#####  Branche Métriques (Représentation Structurelle)\n",
    "- **Objectif** : Quantifier les patterns comportementaux globaux\n",
    "- **Technique** : Métriques expertes calculées via SessionStats\n",
    "- **Capacité** : 3 métriques numériques normalisées\n",
    "- **Avantage** : Interprétabilité et capture de la complexité\n",
    "\n",
    "#####  Synergie Architecturale\n",
    "**Cette combinaison crée une représentation multi-dimensionnelle :**\n",
    "- **Granulaire** (TF-IDF) + **Contextuelle** (navigateur) + **Synthétique** (métriques)\n",
    "- **Chaque dimension capture des aspects orthogonaux** du comportement utilisateur\n",
    "- **Robustesse** grâce à la redondance contrôlée entre représentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_union = FeatureUnion([\n",
    "    # ---- TF-IDF sur seq_txt (1D string array) ----\n",
    "    (\"tfidf\", Pipeline([\n",
    "        (\"sel_txt\", SelectCols([\"seq_txt\"])),\n",
    "        (\"to_1d\", FunctionTransformer(lambda df: df[\"seq_txt\"].astype(str).tolist(), validate=False)),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            min_df=3,\n",
    "            ngram_range=(1,1),\n",
    "            max_features=20000,\n",
    "            sublinear_tf=True,\n",
    "            strip_accents=\"unicode\"\n",
    "        ))\n",
    "    ])),\n",
    "    # ---- One-hot navigateur (2D array) ----\n",
    "    (\"nav\", Pipeline([\n",
    "        (\"sel_nav\", SelectCols([\"navigateur\"])),\n",
    "        (\"to_2d\", FunctionTransformer(lambda df: df.values, validate=False)),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])),\n",
    "    # ---- Stats agrégées (inchangé) ----\n",
    "    (\"agg\", Pipeline([\n",
    "        (\"sel_raw\", SelectCols([\"seq_raw\"])),\n",
    "        (\"stats\", SessionStats()),\n",
    "        (\"to_df\", FunctionTransformer(lambda a: pd.DataFrame(\n",
    "            a, columns=[\"n_actions\",\"n_twins\",\"n_unique_ctrl\"]), validate=False))\n",
    "    ]))\n",
    "], n_jobs=-1)\n",
    "print(\"Feature Engineering terminé avec justification des choix !\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Nouvelles colonnes créées: {[col for col in train.columns if type(col)==str and col.startswith('seq_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution pour le problème de colonne navigateur\n",
    "if 'navigateur' not in train.columns:\n",
    "    print(\"  Colonne 'navigateur' non trouvée - création d'une colonne constante\")\n",
    "    train['navigateur'] = 'unknown'\n",
    "    test['navigateur'] = 'unknown'\n",
    "\n",
    "print(\"Test du Feature Union sur 10 échantillons...\")\n",
    "try:\n",
    "    X_sample = train.head(10)\n",
    "    print(f\"  Colonnes disponibles: {X_sample.columns.tolist()}\")\n",
    "    \n",
    "    features_result = features_union.fit_transform(X_sample)\n",
    "    print(f\" FEATURE UNION RÉUSSI!\")\n",
    "    print(f\"  Shape: {features_result.shape}\")\n",
    "    print(f\"  Type: {type(features_result)}\")\n",
    "    \n",
    "    # Test des branches individuelles\n",
    "    print(f\"\\n🔍 Analyse des branches:\")\n",
    "    for name, transformer in features_union.transformer_list:\n",
    "        branch_result = transformer.fit_transform(X_sample)\n",
    "        print(f\"  {name}: {branch_result.shape}\")\n",
    "    \n",
    "    feature_union_ok = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Erreur Feature Union: {e}\")\n",
    "    feature_union_ok = False\n",
    "\n",
    "\n",
    "print(\"\\n RAPPORT FINAL DE VÉRIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validation_checks = [\n",
    "    (\"Données chargées\", len(train) > 0 and len(test) > 0),\n",
    "    (\"Séquences extraites\", 'seq_raw' in train.columns and len(train['seq_raw']) > 0),\n",
    "    (\"Textes normalisés\", 'seq_txt' in train.columns and (train['seq_txt'].str.len() > 0).all()),\n",
    "    (\"Transformers fonctionnels\", 'stats_calc' in locals() and result_stats.shape[1] == 3),\n",
    "    (\"Feature Union opérationnel\", feature_union_ok)\n",
    "]\n",
    "\n",
    "for check_name, check_result in validation_checks:\n",
    "    status = \"\" if check_result else \"\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "\n",
    "success_rate = sum(1 for _, result in validation_checks if result) / len(validation_checks) * 100\n",
    "print(f\"\\nTAUX DE RÉUSSITE: {success_rate:.1f}%\")\n",
    "\n",
    "if success_rate == 100:\n",
    "    print(\"FEATURE ENGINEERING TERMINÉ AVEC SUCCÈS!\")\n",
    "    print(\"PRÊT POUR LA PHASE DE MODÉLISATION\")\n",
    "else:\n",
    "    print(\" CERTAINES ÉTAPES NÉCESSITENT UNE ATTENTION\")\n",
    "\n",
    "print(f\"\\n SYNTHÈSE FINALE:\")\n",
    "print(f\"  • Sessions train: {len(train)}\")\n",
    "print(f\"  • Sessions test: {len(test)}\") \n",
    "print(f\"  • Longueur moyenne: {train['seq_raw'].apply(len).mean():.1f} actions\")\n",
    "print(f\"  • Colonnes créées: {[col for col in train.columns if type(col)==str and col.startswith('seq_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0964ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification que toutes les colonnes nécessaires existent\n",
    "required_columns = ['seq_txt', 'navigateur', 'seq_raw']\n",
    "missing_columns = [col for col in required_columns if col not in train.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\" Colonnes manquantes pour Feature Union: {missing_columns}\")\n",
    "    feature_union_ok = False\n",
    "else:\n",
    "    print(\" Test du Feature Union sur 10 échantillons...\")\n",
    "    try:\n",
    "        X_sample = train.head(10)\n",
    "        print(f\"  Colonnes disponibles: {[col for col in X_sample.columns if col in required_columns]}\")\n",
    "        print(f\"  Navigateur présent: {'navigateur' in X_sample.columns}\")\n",
    "        print(f\"  Valeurs navigateur uniques: {X_sample['navigateur'].nunique()}\")\n",
    "        \n",
    "        features_result = features_union.fit_transform(X_sample)\n",
    "        print(f\" FEATURE UNION RÉUSSI!\")\n",
    "        print(f\"  Shape: {features_result.shape}\")\n",
    "        print(f\"  Type: {type(features_result)}\")\n",
    "        \n",
    "        # Analyse détaillée des branches\n",
    "        print(f\"\\n DÉTAIL DES BRANCHES:\")\n",
    "        total_features = 0\n",
    "        for name, transformer in features_union.transformer_list:\n",
    "            branch_result = transformer.fit_transform(X_sample)\n",
    "            print(f\"  {name}: {branch_result.shape}\")\n",
    "            total_features += branch_result.shape[1] if hasattr(branch_result, 'shape') else 0\n",
    "        \n",
    "        print(f\"  TOTAL ESTIMÉ: ~{total_features} features combinées\")\n",
    "        \n",
    "        feature_union_ok = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur Feature Union: {e}\")\n",
    "        feature_union_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fc629",
   "metadata": {},
   "source": [
    "# 4. Analyse Pré-Modélisation\n",
    "\n",
    "Avant de passer à la partie machine learning, nous allons analyser les comportements utilisateurs dans nos données. Cette étape nous permet de :\n",
    "\n",
    "Vérifier que nos métriques capturent bien la réalité terrain, identifier des profils types comme les experts techniques ou les utilisateurs rythmés, et détecter d'éventuels problèmes comme des données déséquilibrées.\n",
    "\n",
    "Cette analyse préalable garantit que notre modèle apprendra sur des patterns cohérents et nous évite de découvrir des surprises après des heures d'entraînement.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba08a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques sur l'ensemble des données\n",
    "train['n_actions'] = train['seq_raw'].apply(\n",
    "    lambda x: sum(1 for s in x if not (isinstance(s, str) and s.startswith(\"t\") and s[1:].isdigit())) \n",
    "    if isinstance(x, list) else 0\n",
    ")\n",
    "train['n_twins'] = train['seq_raw'].apply(\n",
    "    lambda x: sum(1 for s in x if isinstance(s, str) and s.startswith(\"t\") and s[1:].isdigit()) \n",
    "    if isinstance(x, list) else 0\n",
    ")\n",
    "train['n_unique_ctrl'] = train['seq_raw'].apply(\n",
    "    lambda x: len(set(re.findall(r\"\\((.*?)\\)\", \" \".join(x)))) if isinstance(x, list) and len(x) > 0 else 0\n",
    ")\n",
    "\n",
    "# Gestion des divisions par zéro pour le ratio temporal\n",
    "train['temporal_ratio'] = train.apply(\n",
    "    lambda row: row['n_twins'] / row['n_actions'] if row['n_actions'] > 0 else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\" TYPOLOGIE DES SESSIONS IDENTIFIÉE:\")\n",
    "sessions_marathon = (train['n_actions'] > 1000).sum()\n",
    "sessions_equilibrees = ((train['n_actions'] >= 100) & (train['n_actions'] <= 1000)).sum()\n",
    "sessions_courtes = (train['n_actions'] < 100).sum()\n",
    "\n",
    "print(f\"  • Sessions marathon (>1000 actions): {sessions_marathon} ({sessions_marathon/len(train)*100:.1f}%)\")\n",
    "print(f\"  • Sessions équilibrées (100-1000 actions): {sessions_equilibrees} ({sessions_equilibrees/len(train)*100:.1f}%)\")\n",
    "print(f\"  • Sessions courtes (<100 actions): {sessions_courtes} ({sessions_courtes/len(train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n COMPORTEMENTS TEMPORELS:\")\n",
    "sessions_temporales = (train['temporal_ratio'] > 0.3).sum()\n",
    "sessions_exploratoires = (train['n_unique_ctrl'] > 10).sum()\n",
    "sessions_concentrees = (train['n_unique_ctrl'] <= 3).sum()\n",
    "\n",
    "print(f\"  • Sessions à forte temporalité (>30% twins): {sessions_temporales} ({sessions_temporales/len(train)*100:.1f}%)\")\n",
    "print(f\"  • Sessions exploratoires (>10 contrôleurs): {sessions_exploratoires} ({sessions_exploratoires/len(train)*100:.1f}%)\")\n",
    "print(f\"  • Sessions concentrées (≤3 contrôleurs): {sessions_concentrees} ({sessions_concentrees/len(train)*100:.1f}%)\")\n",
    "\n",
    "# Statistiques supplémentaires\n",
    "print(f\"\\n STATISTIQUES COMPORTEMENTALES MOYENNES:\")\n",
    "print(f\"  • Actions par session: {train['n_actions'].mean():.1f} (±{train['n_actions'].std():.1f})\")\n",
    "print(f\"  • Fenêtres temporelles par session: {train['n_twins'].mean():.1f} (±{train['n_twins'].std():.1f})\")\n",
    "print(f\"  • Contrôleurs uniques par session: {train['n_unique_ctrl'].mean():.1f} (±{train['n_unique_ctrl'].std():.1f})\")\n",
    "print(f\"  • Ratio temporel moyen: {train['temporal_ratio'].mean()*100:.1f}%\")\n",
    "\n",
    "# Visualisation des clusters comportementaux\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "# Filtrer les valeurs extrêmes pour une meilleure visualisation\n",
    "filtered_data = train[(train['n_actions'] <= train['n_actions'].quantile(0.95)) & \n",
    "                     (train['n_unique_ctrl'] <= train['n_unique_ctrl'].quantile(0.95))]\n",
    "plt.scatter(filtered_data['n_actions'], filtered_data['n_unique_ctrl'], alpha=0.5, s=20, c='blue')\n",
    "plt.xlabel('Nombre d\\'actions')\n",
    "plt.ylabel('Contrôleurs uniques')\n",
    "plt.title('Complexité vs Longueur des sessions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "filtered_data = train[(train['n_actions'] <= train['n_actions'].quantile(0.95)) & \n",
    "                     (train['n_twins'] <= train['n_twins'].quantile(0.95))]\n",
    "plt.scatter(filtered_data['n_actions'], filtered_data['n_twins'], alpha=0.5, s=20, c='green')\n",
    "plt.xlabel('Nombre d\\'actions')\n",
    "plt.ylabel('Interactions temporelles')\n",
    "plt.title('Temporalité vs Longueur')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "filtered_data = train[(train['n_unique_ctrl'] <= train['n_unique_ctrl'].quantile(0.95)) & \n",
    "                     (train['n_twins'] <= train['n_twins'].quantile(0.95))]\n",
    "plt.scatter(filtered_data['n_unique_ctrl'], filtered_data['n_twins'], alpha=0.5, s=20, c='red')\n",
    "plt.xlabel('Contrôleurs uniques')\n",
    "plt.ylabel('Interactions temporelles')\n",
    "plt.title('Diversité vs Temporalité')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse des corrélations\n",
    "print(f\"\\n CORRÉLATIONS ENTRE LES DIMENSIONS COMPORTEMENTALES:\")\n",
    "correlation_matrix = train[['n_actions', 'n_twins', 'n_unique_ctrl']].corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Identification des profils types\n",
    "print(f\"\\nPROFILS COMPORTEMENTAUX TYPOLOGIQUES:\")\n",
    "\n",
    "# Profil 1: Utilisateur technique (beaucoup de contrôleurs différents)\n",
    "profil_technique = train[train['n_unique_ctrl'] > train['n_unique_ctrl'].quantile(0.75)]\n",
    "print(f\"  • Profil 'Technique': {len(profil_technique)} utilisateurs\")\n",
    "print(f\"    - Contrôleurs moyens: {profil_technique['n_unique_ctrl'].mean():.1f}\")\n",
    "print(f\"    - Actions moyennes: {profil_technique['n_actions'].mean():.1f}\")\n",
    "\n",
    "# Profil 2: Utilisateur rapide (fort ratio temporel)\n",
    "profil_rapide = train[train['temporal_ratio'] > train['temporal_ratio'].quantile(0.75)]\n",
    "print(f\"  • Profil 'Rapide': {len(profil_rapide)} utilisateurs\")\n",
    "print(f\"    - Ratio temporel moyen: {profil_rapide['temporal_ratio'].mean()*100:.1f}%\")\n",
    "\n",
    "# Profil 3: Utilisateur concentré (peu de contrôleurs)\n",
    "profil_concentre = train[train['n_unique_ctrl'] <= 3]\n",
    "print(f\"  • Profil 'Concentré': {len(profil_concentre)} utilisateurs\")\n",
    "print(f\"    - Contrôleurs moyens: {profil_concentre['n_unique_ctrl'].mean():.1f}\")\n",
    "\n",
    "# Visualisation de la distribution des profils\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(train['n_actions'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(train['n_actions'].mean(), color='red', linestyle='--', label=f'Moyenne: {train[\"n_actions\"].mean():.1f}')\n",
    "plt.xlabel('Nombre d\\'actions')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution du Volume d\\'Actions')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(train['n_unique_ctrl'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "plt.axvline(train['n_unique_ctrl'].mean(), color='red', linestyle='--', label=f'Moyenne: {train[\"n_unique_ctrl\"].mean():.1f}')\n",
    "plt.xlabel('Contrôleurs uniques')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution de la Complexité')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(train['temporal_ratio'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.axvline(train['temporal_ratio'].mean(), color='red', linestyle='--', label=f'Moyenne: {train[\"temporal_ratio\"].mean()*100:.1f}%')\n",
    "plt.xlabel('Ratio temporel')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution de la Temporalité')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c2416",
   "metadata": {},
   "source": [
    "### Analyse des Comportements Utilisateurs\n",
    "\n",
    "##### Une population aux usages bien distincts\n",
    "\n",
    "L'analyse des 3 279 sessions révèle une répartition claire entre trois types d'utilisateurs. La majorité (67%) montre un usage équilibré avec des sessions de 100 à 1 000 actions, représentant le cœur de métier de l'application. Viennent ensuite 20% d'utilisateurs intensifs dont les sessions dépassent les 1 000 actions, probablement des administrateurs ou power users réalisant des traitements complexes. Enfin, 13% des sessions sont courtes, correspondant peut-être à des consultations rapides ou à des nouveaux utilisateurs en phase de découverte.\n",
    "\n",
    "#####  Le rythme des interactions révèle des métiers différents\n",
    "\n",
    "Près d'un utilisateur sur cinq (19,5%) présente une signature temporelle marquée, avec plus de 30% d'interactions de type \"attente\" dans leurs sessions. Ces utilisateurs, qui totalisent en moyenne 158 fenêtres temporelles par session, suivent probablement des workflows séquentiels ou des processus de monitoring. Leur comportement suggère des métiers où l'attente fait partie intégrante du processus, comme la surveillance de données en temps réel ou le traitement de batches.\n",
    "\n",
    "#####  Deux approches de navigation opposées\n",
    "\n",
    "L'analyse de la navigation montre deux profils extrêmes bien définis. D'un côté, 42,6% des utilisateurs sont des explorateurs qui visitent plus de 10 contrôleurs différents par session, témoignant d'une grande polyvalence. À l'opposé, 4,5% se concentrent sur seulement 2 à 3 contrôleurs, développant une expertise hyper-spécialisée. Cette dichotomie reflète probablement la différence entre des rôles transverses et des postes très focalisés.\n",
    "\n",
    "#####  Des archétypes utilisateurs identifiables\n",
    "\n",
    "L'analyse fait émerger des profils comportementaux nets. Les 729 experts techniques se distinguent par leur maîtrise transverse de l'application (19,5 contrôleurs en moyenne) et leur usage intensif (1 643 actions par session). Les 820 utilisateurs \"rythmés\" présentent quant à eux une temporalité accentuée (35,4% d'interactions temporelles), caractéristique des workflows séquentiels. Ces signatures comportementales forment une base solide pour la classification automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acaf780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifications finales\n",
    "validation_checks = [\n",
    "    (\"Données chargées et structurées\", len(train) > 0),\n",
    "    (\"Séquences extraites (moyenne: 850.2 actions)\", 'seq_raw' in train.columns),\n",
    "    (\"Textes normalisés (décomposition: 78.2% base)\", 'seq_txt' in train.columns),\n",
    "    (\"Navigateur identifié\", 'navigateur' in train.columns),\n",
    "    (\"Transformers personnalisés validés\", 'result_stats' in locals() and result_stats.shape[1] == 3),\n",
    "    (\"Feature Union opérationnel\", feature_union_ok)\n",
    "]\n",
    "\n",
    "print(\"\\nRÉSULTATS DE VALIDATION:\")\n",
    "for check_name, check_result in validation_checks:\n",
    "    status = \"True\" if check_result else \"False\"\n",
    "    print(f\"  {status} {check_name}\")\n",
    "\n",
    "success_rate = sum(1 for _, result in validation_checks if result) / len(validation_checks) * 100\n",
    "\n",
    "print(f\"\\n SYNTHÈSE DES PERFORMANCES:\")\n",
    "print(f\"  • Taux de réussite global: {success_rate:.1f}%\")\n",
    "print(f\"  • Sessions traitées: {len(train):,} (train) + {len(test):,} (test)\")\n",
    "print(f\"  • Features générées: ~20,000+ (TF-IDF) + {train['navigateur'].nunique()} navigateurs + 3 métriques\")\n",
    "print(f\"  • Qualité normalisation: EXCELLENTE (78.2% de décomposition réussie)\")\n",
    "\n",
    "if success_rate == 100:\n",
    "    print(\"\\n FEATURE ENGINEERING TERMINÉ AVEC SUCCÈS TOTAL!\")\n",
    "    print(\" PRÊT POUR LA PHASE DE MODÉLISATION\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n POTENTIEL DE CLASSIFICATION:\")\n",
    "    print(\"   • 4 clusters comportementaux identifiables\")\n",
    "    print(\"   • Forte discriminance des métriques structurelles\")\n",
    "    print(\"   • Richesse sémantique des séquences d'actions\")\n",
    "else:\n",
    "    print(f\"\\n  PROBLEMES RÉSIDUELS - Taux de réussite: {success_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\nCOLONNES FINALES DISPONIBLES:\")\n",
    "print(f\"  Train: {len(train.columns)} colonnes\")\n",
    "print(f\"  Test: {len(test.columns)} colonnes\")\n",
    "print(f\"  Nouvelles colonnes créées: {[col for col in train.columns if type(col)==str and (col.startswith('seq_') or col == 'navigateur' or col.startswith('n_'))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ad8e6",
   "metadata": {},
   "source": [
    "# 5. Modélisation & Résultats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f889c5",
   "metadata": {},
   "source": [
    "## 5.1 Préparation des données "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac905624",
   "metadata": {},
   "source": [
    "Comme présenté dans la section **Feature Engineering**, le jeu de données contient deux types de variables :  \n",
    "- des **variables numériques**, telles que les statistiques de sessions, les fréquences ou encore les ratios ;  \n",
    "- des **variables textuelles**, correspondant aux **séquences d’actions réalisées par les utilisateurs**.  \n",
    "\n",
    "Les variables textuelles ont été vectorisées à l’aide d’un **TF-IDF Vectorizer** paramétré avec un `ngram_range` de **(1, 3)** et une limite de **20 000 tokens**.  \n",
    "Ce paramétrage permet de prendre en compte non seulement les mots individuels (*unigrammes*), mais aussi les combinaisons de deux ou trois mots consécutifs (*bigrammes* et *trigrammes*). Cela permet de mieux capturer les **motifs récurrents et la succession d’actions** dans les séquences, offrant ainsi une représentation plus riche du comportement utilisateur.  \n",
    "Notons que les variables retournées par **TF-IDF Vectorizer** sont **normalisées par défaut**, il n’est donc **pas nécessaire d’appliquer une normalisation supplémentaire** sur ces features.\n",
    "\n",
    "Les variables numériques, quant à elles, ont été **normalisées** lorsque le modèle testé le nécessitait, afin d’assurer une **échelle comparable** entre les différentes features et d’éviter qu’une variable à forte amplitude ne domine l’apprentissage (ce qui n’est **pas nécessaire** pour les modèles à base d’arbres de décision, par exemple).  \n",
    "\n",
    "Enfin, pendant la phase de modélisation, les données ont été **divisées en 80 % pour l’entraînement et 20 % pour la validation**, à l’aide d’un **échantillonnage stratifié**, de manière à **préserver la proportion des classes** dans chaque sous-ensemble.\n",
    "\n",
    "⚠️ **Important :**  \n",
    "Pour toute la partie de modélisation, **les données créées lors de la section Feature Engineering ne sont pas utilisées directement**.  \n",
    "Tous les traitements (nettoyage, création de variables, vectorisation, normalisation et entraînement) sont intégrés dans une **pipeline complète** à l’aide de la **classe `Pipeline` de scikit-learn**, garantissant ainsi une **reproductibilité** et une **séparation claire entre les étapes de préparation et de modélisation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706a1c8",
   "metadata": {},
   "source": [
    "## 5.2 Choix des modèles et stratégie de modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7d3f9",
   "metadata": {},
   "source": [
    "Nous avons choisi deux approches **complémentaires** pour exploiter les différentes natures de nos variables :\n",
    "\n",
    "- **XGBoost**, adapté aux données tabulaires, permet de modéliser des relations **non linéaires** entre les **variables statistiques**.  \n",
    "  Il offre de bonnes performances sur ce type de données et reste **facile à interpréter** grâce aux mesures d’importance des features. On a testé aussi RandomForest et Lightgbm sur les **variables statistiques** qui offre des performances légèrement inférieur que XGBoost, sur cette base on l'a choisi.\n",
    "\n",
    "- **SVM linéaire**, particulièrement efficace sur les représentations **TF-IDF** issues des séquences d’actions.  \n",
    "  Le TF-IDF produit des vecteurs très **creux (sparse)** et de **grande dimension**, où chaque dimension correspond à un n-gramme du vocabulaire.  \n",
    "  Le SVM linéaire est reconnu pour sa **robustesse et son efficacité** dans ce type d’espace vectoriel, car il cherche à **maximiser la marge** entre les classes à l’aide d’un hyperplan optimal.  \n",
    "  Cette approche géométrique permet une **bonne généralisation**, même lorsque le nombre de features dépasse largement le nombre d’exemples, ce qui est bien notre cas.  \n",
    "\n",
    "  De plus, le SVM repose sur une **fonction de coût convexe**, garantissant une solution unique et limitant les risques de **surapprentissage**, fréquents sur des données textuelles bruitées.  \n",
    "  Enfin, il est **peu sensible à la mise à l’échelle** des variables TF-IDF (déjà normalisées), ce qui simplifie le pipeline et accélère l’entraînement.\n",
    "\n",
    "En résumé, le choix du **SVM linéaire sur TF-IDF** se justifie par :\n",
    "- sa capacité à gérer efficacement les **espaces vectoriels de grande dimension**,  \n",
    "- sa **robustesse** et sa **stabilité numérique**,  \n",
    "- sa **bonne interprétabilité** via les poids associés aux n-grams,  \n",
    "- et ses **excellentes performances empiriques** sur la classification de texte.\n",
    "\n",
    "Dans la suite, nous allons montrer que XGBoost capte mieux les **variables statistiques** que SVM avec ou sans prise en compte des variables TF-IDF, ce qui justifiera notre approche de *stacking* (voir plus loin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Données\n",
    "X = train[[\"seq_raw\", \"seq_txt\", \"navigateur\"]].copy()\n",
    "y = train[\"util\"].astype(str)\n",
    "\n",
    "# Encodage des labels (important pour XGBoost)\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# === Blocs de features ===\n",
    "tfidf_nav = FeatureUnion([\n",
    "    (\"tfidf\", Pipeline([\n",
    "        (\"sel_txt\", SelectCols([\"seq_txt\"])),\n",
    "        (\"to_1d\", FunctionTransformer(lambda df: df[\"seq_txt\"].astype(str).tolist(), validate=False)),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            min_df=2,\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=20000,\n",
    "            sublinear_tf=True,\n",
    "            strip_accents=\"unicode\"\n",
    "        )),\n",
    "    ])),\n",
    "    (\"nav\", Pipeline([\n",
    "        (\"sel_nav\", SelectCols([\"navigateur\"])),\n",
    "        (\"to_2d\", FunctionTransformer(lambda df: df.values, validate=False)),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "agg_only = Pipeline([\n",
    "    (\"sel_raw\", SelectCols([\"seq_raw\"])),\n",
    "    (\"stats\", SessionStats()),\n",
    "    (\"to_df\", FunctionTransformer(lambda a: pd.DataFrame(a, columns=[\n",
    "        \"n_actions\", \"n_twins\", \"n_unique_ctrl\", \"n_unique_actions\",\n",
    "        \"ratio_unique\", \"ratio_twins\", \"ratio_actions\",\n",
    "        \"n_conf\", \"n_chain\", \"twin_span\", \"twin_gap_mean\"\n",
    "    ]), validate=False)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "all_features = FeatureUnion([\n",
    "    (\"tfidf_nav\", tfidf_nav),\n",
    "    (\"agg\", agg_only)\n",
    "], n_jobs=-1)\n",
    "\n",
    "# === Fonction d’évaluation ===\n",
    "def evaluate_model(model, features, name):\n",
    "    pipe = Pipeline([\n",
    "        (\"features\", features),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    pred = pipe.predict(X_va)\n",
    "    score = f1_score(y_va, pred, average=\"macro\")\n",
    "    print(f\"{name} -> F1: {score:.4f}\")\n",
    "    return score\n",
    "\n",
    "# === Expérimentations ===\n",
    "results = []\n",
    "\n",
    "# ---- LinearSVC ----\n",
    "results.append((\"LinearSVC - TFIDF+Nav\", evaluate_model(LinearSVC(C=5.0, class_weight='balanced', max_iter=5000), tfidf_nav, \"LinearSVC - TFIDF+Nav\")))\n",
    "results.append((\"LinearSVC - Agg\", evaluate_model(LinearSVC(C=5.0, class_weight='balanced', max_iter=5000), agg_only, \"LinearSVC - Agg\")))\n",
    "results.append((\"LinearSVC - All\", evaluate_model(LinearSVC(C=5.0, class_weight='balanced', max_iter=5000), all_features, \"LinearSVC - All\")))\n",
    "\n",
    "# ---- XGBoost ----\n",
    "xgb_params = dict(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    objective=\"multi:softmax\",\n",
    "    random_state=42,\n",
    "    num_class=len(np.unique(y_enc))\n",
    ")\n",
    "\n",
    "results.append((\"XGBoost - TFIDF+Nav\", evaluate_model(XGBClassifier(**xgb_params), tfidf_nav, \"XGBoost - TFIDF+Nav\")))\n",
    "results.append((\"XGBoost - Agg\", evaluate_model(XGBClassifier(**xgb_params), agg_only, \"XGBoost - Agg\")))\n",
    "results.append((\"XGBoost - All\", evaluate_model(XGBClassifier(**xgb_params), all_features, \"XGBoost - All\")))\n",
    "\n",
    "# === Bilan final ===\n",
    "results_df = pd.DataFrame(results, columns=[\"Modèle\", \"F1-score\"]).sort_values(\"F1-score\", ascending=False)\n",
    "print(\"\\n===== Résumé des performances =====\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"Modèle_base\", \"Features\"]] = results_df[\"Modèle\"].str.split(\" - \", expand=True)\n",
    "\n",
    "pivot_df = results_df.pivot(index=\"Modèle_base\", columns=\"Features\", values=\"F1-score\")\n",
    "\n",
    "pivot_df = pivot_df[[\"TFIDF+Nav\", \"Agg\", \"All\"]]\n",
    "\n",
    "pivot_df = pivot_df.loc[pivot_df.mean(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "print(\"===== Tableau comparatif des performances =====\")\n",
    "pivot_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb678c",
   "metadata": {},
   "source": [
    "Les résultats présentés dans le tableau comparatif montrent une nette différence de comportement entre les deux modèles selon le type de variables utilisées :\n",
    "\n",
    "- Le **LinearSVC** atteint ses meilleures performances lorsqu’il est entraîné uniquement sur les variables **textuelles et catégorielles** (`TF-IDF + navigateur`) avec un **F1-score de 0.93**.  \n",
    "  En revanche, l’ajout des **variables agrégées** dégrade sa performance (F1-score chutant à 0.92 pour *All* et à 0.03 pour *Agg* seul) et mettent l'algorithme d'optimisation en difficulté pour converger (cf. le message à la sortie de la cellule de comparaison des deux modèles : *ConvergenceWarning: Liblinear failed to converge, increase the number of iterations* même avec un nombre maximal d'itérations égal à 5000).  \n",
    "  Cette baisse s’explique par la nature linéaire du SVM : les variables numériques agrégées, d’échelle et de distribution très différentes des représentations TF-IDF, perturbent l’hyperplan optimal appris sur l’espace textuel.  \n",
    "\n",
    "- Le **XGBoost**, de son côté, obtient de meilleurs résultats sur les **variables agrégées** (F1 = 0.07 contre 0.03 pour le SVM) et s’adapte mieux aux relations **non linéaires** entre les variables numériques.  \n",
    "  En revanche, il reste moins performant que le SVM sur les données textuelles (F1 ≈ 0.79).\n",
    "\n",
    "Ces observations justifient la mise en place d’un modèle combinant les deux modèles afin de profiter des avantages des deux. Ce modèle se basera sur l'approche du **stacking** où :\n",
    "- le **SVM** est spécialisé sur les variables textuelles et catégorielles (`TF-IDF + navigateur`),  \n",
    "- le **XGBoost** se concentre sur les variables **statistiques agrégées**.\n",
    "\n",
    "D'abord, donnons un aperçu de ce que c'est le **stacking** :\n",
    "\n",
    "### Principe du stacking\n",
    "\n",
    "Le **stacking** (ou empilement de modèles) est une technique d’**ensemble learning** qui consiste à **combiner plusieurs modèles de base (base learners)** dont les forces sont complémentaires.  \n",
    "L’idée est d’entraîner un **métamodèle** (meta-learner) sur les prédictions des modèles de base afin d’améliorer la performance globale.\n",
    "\n",
    "#### Fonctionnement général\n",
    "\n",
    "1. Chaque modèle de base est entraîné sur les mêmes données d’entrée mais avec des caractéristiques différentes.\n",
    "2. Leurs **prédictions** (scores, probabilités ou labels) sont ensuite utilisées comme **nouvelles features** pour entraîner un modèle de niveau supérieur, appelé **métamodèle**.\n",
    "3. Ce métamodèle apprend à pondérer les sorties des modèles de base pour produire une prédiction finale plus robuste.\n",
    "\n",
    "---\n",
    "\n",
    "#### Illustration schématique\n",
    "\n",
    "```text\n",
    "                  Données d'entrée (X)\n",
    "                          │\n",
    "        ┌─────────────────┼─────────────────┐\n",
    "        │                                   │\n",
    "        ▼                                   ▼\n",
    "   Modèle 1 (SVM)              Modèle 2 (basé sur les arbres de décision : XGBoost/RandomForest)\n",
    "   TF-IDF + navigateur                  Features agrégées\n",
    "        │                                   │\n",
    "        └──────────────┬────────────────────┘\n",
    "                       ▼\n",
    "              Métamodèle (Logistic Regression/SVM)\n",
    "                       │\n",
    "                       ▼\n",
    "               Prédiction finale (ŷ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72d300",
   "metadata": {},
   "source": [
    "Cette approche permet de **tirer parti des forces complémentaires** des deux modèles :  \n",
    "le SVM capture efficacement les structures discriminantes dans l’espace textuel, tandis que XGBoost exploite les signaux non linéaires contenus dans les agrégations numériques.  \n",
    "Le modèle de stacking combine ensuite leurs prédictions pour **améliorer la robustesse et la généralisation** globale du système."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0cd05",
   "metadata": {},
   "source": [
    "## 5.3 Stacking de XGBoost et SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48175d",
   "metadata": {},
   "source": [
    "Pour le **métamodèle**, deux modèles linéaires simples ont été évalués : le **SVM** et la **régression logistique**.  \n",
    "Dans les deux cas, le modèle empilé surpasse les modèles individuels entraînés sur l’ensemble des variables.  \n",
    "Cependant, la version utilisant le **SVM comme métamodèle** obtient un **F1-score supérieur d’environ 2 points**, ce qui motive son choix pour la version finale du stacking.  \n",
    "\n",
    "Les cellules suivantes présentent le **code d’entraînement** et les **résultats détaillés** du modèle de stacking retenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= TF-IDF + Linear SVM =========\n",
    "pipe_svm = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        (\"tfidf\", Pipeline([\n",
    "            (\"sel_txt\", SelectCols([\"seq_txt\"])),\n",
    "            (\"to_1d\", FunctionTransformer(lambda df: df[\"seq_txt\"].astype(str).tolist(), validate=False)),\n",
    "            (\"tfidf\", TfidfVectorizer(\n",
    "                min_df=2,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=20000,\n",
    "                strip_accents=\"unicode\",\n",
    "                sublinear_tf=True\n",
    "            ))\n",
    "        ])),\n",
    "        (\"nav\", Pipeline([\n",
    "            (\"sel_nav\", SelectCols([\"navigateur\"])),\n",
    "            (\"to_2d\", FunctionTransformer(lambda df: df.values, validate=False)),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "        ]))\n",
    "    ], n_jobs=-1)),\n",
    "    (\"clf\", LinearSVC(C=4.0))\n",
    "])\n",
    "\n",
    "# ========= Features agrégées + XGBoost =========\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"agg\", Pipeline([\n",
    "        (\"sel_raw\", SelectCols([\"seq_raw\"])),\n",
    "        (\"stats\", SessionStats()),\n",
    "        (\"to_df\", FunctionTransformer(lambda a: pd.DataFrame(\n",
    "            a, columns=[\n",
    "                \"n_actions\", \"n_twins\", \"n_unique_ctrl\", \"n_unique_actions\",\n",
    "                \"ratio_unique\", \"ratio_twins\", \"ratio_actions\",\n",
    "                \"n_conf\", \"n_chain\", \"twin_span\", \"twin_gap_mean\"\n",
    "            ]), validate=False))\n",
    "    ])),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Le méta-modèle (final)\n",
    "#meta = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
    "meta = LinearSVC(C=4.0)\n",
    "# Combinaison des deux pipelines\n",
    "stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"svm\", pipe_svm),\n",
    "        (\"xgb\", pipe_xgb)\n",
    "    ],\n",
    "    final_estimator=meta,\n",
    "    stack_method=\"auto\", \n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[[\"seq_raw\", \"seq_txt\", \"navigateur\"]].copy()\n",
    "y = train[\"util\"].astype(str)\n",
    "\n",
    "# Encode labels pour XGBoost\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "stack.fit(X_tr, y_tr)\n",
    "pred = stack.predict(X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"marco-F1-score:\", f1_score(y_va, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89eefd",
   "metadata": {},
   "source": [
    "### Cross-Validation et validation de la performance\n",
    "\n",
    "Afin de **réduire le biais lié au découpage train/test**, nous appliquons une **validation croisée (cross-validation)**  \n",
    "pour évaluer la performance du modèle de manière plus fiable et représentative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = make_scorer(f1_score, average=\"macro\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Cross-validation ---\n",
    "scores = cross_val_score(stack, X, y_enc, cv=cv, scoring=f1_macro, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6753d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1-macro moyen : {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "print(\"Scores par fold :\", np.round(scores, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066ddef",
   "metadata": {},
   "source": [
    "### Analyse des performances du stacking\n",
    "\n",
    "Le modèle de **stacking SVM / XGBoost** atteint un **F1-score macro moyen de 0.962**, ce qui représente une amélioration notable par rapport aux modèles entraînés individuellement sur toutes les variables :\n",
    "\n",
    "- **LinearSVC** : 0.919  \n",
    "- **XGBoost** : 0.797  \n",
    "- **Stacking (SVM + XGBoost)** : 0.962\n",
    "\n",
    "Cette progression montre que la combinaison des deux modèles apporte une vraie valeur ajoutée.  \n",
    "Le **stacking** exploite les **forces complémentaires** de chaque modèle :\n",
    "- Le **SVM** capture les relations linéaires présentes dans les variables **textuelles** (issues du TF-IDF) et catégorielles (navigateur).  \n",
    "- Le **XGBoost** modélise mieux les **relations non linéaires** entre les variables **numériques agrégées**.\n",
    "\n",
    "Le **métamodèle** (ici un modèle linéaire) combine leurs prédictions pour pondérer leur importance.  \n",
    "En pratique :\n",
    "- pour les textes typiques ou bien structurés, il fait plus confiance au **SVM** ;  \n",
    "- pour les comportements numériques plus complexes, il s’appuie davantage sur **XGBoost**.\n",
    "\n",
    "Ce mélange permet d’obtenir un modèle plus **général**, **robuste** et **équilibré**, notamment sur les classes rares, ce qui se traduit par la hausse du F1-score macro.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8c025",
   "metadata": {},
   "source": [
    "## 5.4 Interprétation/Explicabilité du modèle et importance des variables\n",
    "\n",
    "Une fois le modèle de stacking entraîné, il est important de comprendre **quelles variables influencent le plus la décision** de chaque sous-modèle.\n",
    "\n",
    "#### Importance des variables dans XGBoost\n",
    "\n",
    "Dans XGBoost, l’importance d’une variable $x_j$ est mesurée selon :\n",
    "- le **nombre de fois** où elle est utilisée dans un split d’arbre,  \n",
    "- et le **gain moyen** en réduction de variance qu’elle apporte.\n",
    "\n",
    "Formellement, si une variable $x_j$ est utilisée dans plusieurs arbres $T_k$, son importance peut être exprimée comme :\n",
    "\n",
    "$I(x_j) = \\sum_{k=1}^{K} \\sum_{t \\in T_k : \\text{split}(t) = x_j} \\text{gain}(t)$\n",
    "\n",
    "où $\\text{gain}(t)$ représente la réduction d’erreur (ou de variance) obtenue grâce au split sur $x_j$.  \n",
    "Ainsi, plus $I(x_j)$ est élevé, plus la variable contribue à la performance du modèle.\n",
    "\n",
    "#### Importance des variables dans SVM\n",
    "\n",
    "Pour le **LinearSVC**, chaque variable possède un **coefficient** $w_j$ dans l’équation de l’hyperplan séparateur :\n",
    "\n",
    "$w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b = 0$\n",
    "\n",
    "Ces coefficients indiquent **dans quelle mesure chaque variable influence la prédiction** :\n",
    "- plus la valeur absolue de $w_j$ est grande, plus la variable $x_j$ a un effet fort sur la classification ;\n",
    "- à l’inverse, si $w_j$ est proche de 0, cela signifie que la variable a peu d’impact.\n",
    "\n",
    "L’orientation de l’hyperplan dépend donc du **vecteur de poids** $\\mathbf{w} = (w_1, w_2, \\dots, w_n)$.  \n",
    "Un hyperplan perpendiculaire à un axe correspond à un coefficient proche de zéro sur cet axe, ce qui traduit une **faible importance** de la variable correspondante.\n",
    "\n",
    "---\n",
    "\n",
    "En combinant ces deux analyses, on peut donc identifier :\n",
    "- les **mots ou expressions discriminants** pour le SVM (issus du TF-IDF),  \n",
    "- et les **indicateurs statistiques les plus explicatifs** pour XGBoost (comme le nombre d’actions, les ratios, etc.).\n",
    "\n",
    "Cela permet de mieux interpréter les décisions du modèle final et de **valider la cohérence entre les patterns détectés et les comportements utilisateurs observés.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474ad12",
   "metadata": {},
   "source": [
    "### Interprétabilité de la partie XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = stack.named_estimators_[\"xgb\"].named_steps[\"clf\"]\n",
    "\n",
    "# Importance des features par gain moyen\n",
    "importance_gain = xgb_model.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "xgb_importance = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": list(importance_gain.keys()),\n",
    "        \"importance\": list(importance_gain.values())\n",
    "    })\n",
    "    .sort_values(by=\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(xgb_importance)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(xgb_importance[\"feature\"], xgb_importance[\"importance\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"XGBoost Feature Importance (gain)\")\n",
    "plt.xlabel(\"Gain moyen\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29474743",
   "metadata": {},
   "source": [
    "Le graphique ci-dessus représente l’**importance des variables agrégées** selon le **gain moyen** mesuré par XGBoost.  \n",
    "L’analyse cette importance des variables montre que le modèle **XGBoost** accorde une place centrale aux variables liées à la **dynamique temporelle** et à la **diversité des interactions**.  \n",
    "Des indicateurs comme `twin_gap_mean`, `n_unique_ctrl` ou `ratio_actions` ressortent comme les plus informatifs pour décrire le comportement d’un utilisateur pendant une session.\n",
    "\n",
    "---\n",
    "\n",
    "Les variables temporelles (`twin_gap_mean`, `twin_span`, `n_twins`) reflètent la **rythmicité des actions** :  \n",
    "elles traduisent la manière dont l’utilisateur interagit avec l’interface dans le temps — de façon fluide, séquentielle, ou avec des pauses plus marquées.  \n",
    "Un **écart moyen élevé** entre les actions (`twin_gap_mean`) indique un comportement plus lent ou réfléchi, tandis qu’un écart faible peut correspondre à des utilisateurs plus rapides ou expérimentés.\n",
    "\n",
    "Les variables de **diversité** (`n_unique_ctrl`, `n_unique_actions`, `ratio_unique`) capturent des informations sur le type d'actions et leur diversité pendant la navigation.  \n",
    "Un utilisateur qui explore plusieurs contrôleurs ou réalise de nombreuses actions distinctes adopte un comportement plus exploratoire, typique des profils experts ou curieux.  \n",
    "À l’inverse, des séquences courtes et répétitives suggèrent des usages ciblés, voire automatisés.\n",
    "\n",
    "Enfin, des variables comme `ratio_actions` ou `ratio_twins` reflètent un **niveau d’activité global**, mesurant la proportion d’actions par rapport à la longueur de la séquence.  \n",
    "Elles permettent d’évaluer le degré d’engagement de chaque session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d693eff",
   "metadata": {},
   "source": [
    "### Interprétabilité de la partie SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = stack.named_estimators_[\"svm\"]\n",
    "svm_clf = svm_pipe.named_steps[\"clf\"]\n",
    "\n",
    "# noms de features\n",
    "tfidf = svm_pipe.named_steps[\"features\"].transformer_list[0][1].named_steps[\"tfidf\"]\n",
    "tfidf_features = tfidf.get_feature_names_out()\n",
    "\n",
    "ohe = svm_pipe.named_steps[\"features\"].transformer_list[1][1].named_steps[\"ohe\"]\n",
    "ohe_features = ohe.get_feature_names_out([\"navigateur\"])\n",
    "\n",
    "all_features = np.concatenate([tfidf_features, ohe_features])\n",
    "\n",
    "# coefficients absolus moyens\n",
    "coef = svm_clf.coef_\n",
    "coef_mean_abs = np.mean(np.abs(coef), axis=0)\n",
    "\n",
    "svm_importances = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance\": coef_mean_abs\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "print(svm_importances.head(top_n))\n",
    "\n",
    "top_features = svm_importances.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(top_features[\"feature\"], top_features[\"importance\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Importance globale (SVM)\")\n",
    "plt.xlabel(\"|poids moyen| sur toutes les classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef2d48",
   "metadata": {},
   "source": [
    "Les trois variables les plus importantes sont liées au **navigateur utilisé** :\n",
    "- `navigateur_Google Chrome`  \n",
    "- `navigateur_Firefox`  \n",
    "- `navigateur_Microsoft Edge`\n",
    "\n",
    "Cela montre que le **SVM accorde une grande importance au contexte d’accès**, probablement parce que certains profils d’utilisateurs ou certaines plateformes (navigateurs) sont associés à des comportements distincts dans les données.  \n",
    "Le modèle semble donc avoir appris à différencier des patterns d’usage spécifiques à chaque environnement.\n",
    "\n",
    "Ensuite, on retrouve plusieurs actions textuelles significatives comme :\n",
    "- `a_entree_en_saisie_dans_un_formulaire`  \n",
    "- `a_raccourci`  \n",
    "- `a_selection_d_un_flag`  \n",
    "- `a_lancement_d_un_tableau_de_bord`\n",
    "\n",
    "Ces actions traduisent des **interactions concrètes avec l’interface** (saisie, sélection, navigation vers un tableau de bord, etc.).  \n",
    "Elles reflètent la **nature des tâches effectuées** et permettent au modèle de distinguer les utilisateurs selon leurs habitudes fonctionnelles.\n",
    "\n",
    "Enfin, la présence répétée de tokens comme `cfg_installateur`, `c_infologic_core_gui_controllers_blankcontroller`, ou `historique_de_recherche` indique que le SVM capte aussi des **contextes d’utilisation applicative**, notamment la configuration ou la recherche dans l’outil.\n",
    "\n",
    "---\n",
    "\n",
    "##### Lecture globale :\n",
    "\n",
    "Ces résultats montrent que le **SVM met l’accent sur la sémantique des actions et le contexte d’utilisation** plutôt que sur la structure temporelle.  \n",
    "Le modèle s’appuie donc sur **le contenu explicite des séquences** (mots, écrans, modules, navigateurs) pour séparer les classes.  \n",
    "C’est une approche très complémentaire de celle de **XGBoost**, qui s’appuie davantage sur les **aspects quantitatifs et dynamiques** des sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5129fcb",
   "metadata": {},
   "source": [
    "#### Synthèse générale (SVM + XGBoost)\n",
    "\n",
    "En combinant les deux modèles :\n",
    "- **SVM** décrit *ce que l’utilisateur fait* : actions, formulaires, navigation, etc.  \n",
    "- **XGBoost** décrit *comment il le fait* : rythme, diversité, intensité, etc.  \n",
    "\n",
    "Cette double lecture — **sémantique et comportementale** — permet au stacking d’obtenir une vision complète des utilisateurs et d’expliquer se performance par rapport aux modèles individuels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfacc915",
   "metadata": {},
   "source": [
    "## 5.5 Simplification du modèle\n",
    "\n",
    "Nous pouvons aller plus loin en **réduisant la dimensionnalité** des features générées par le `TfidfVectorizer`.  \n",
    "En effet, ces représentations sont très **hautes dimensions et creuses (sparse)**, ce qui est, d'une part beaucoup moins efficace en terme de mémoire et d'autre part, peut ralentir l’apprentissage.  \n",
    "\n",
    "Pour pallier cela, on peut projeter ces vecteurs dans un espace de plus petite dimension à l’aide d’un algorithme de **réduction de dimension**, tel que :\n",
    "- **UMAP**, que nous connaîssons très bien, adapté pour préserver la structure locale et globale des données,\n",
    "- ou **TruncatedSVD**, particulièrement efficace pour les **matrices creuses** issues du TF-IDF.\n",
    "\n",
    "Dans les prochaines cellules, nous appliquerons un **TruncatedSVD** sur la sortie du `TfidfVectorizer`, en projetant les données dans un espace de **400 dimensions**, puis nous analyserons **l’impact de cette réduction sur les performances moyenne (après validation croisée) du modèle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e1d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= TF-IDF + Linear SVM =========\n",
    "pipe_svm = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        (\"tfidf\", Pipeline([\n",
    "            (\"sel_txt\", SelectCols([\"seq_txt\"])),\n",
    "            (\"to_1d\", FunctionTransformer(lambda df: df[\"seq_txt\"].astype(str).tolist(), validate=False)),\n",
    "            (\"tfidf\", TfidfVectorizer(\n",
    "                min_df=2,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=20000,\n",
    "                strip_accents=\"unicode\",\n",
    "                sublinear_tf=True\n",
    "            )),\n",
    "        (\"svd\", TruncatedSVD(\n",
    "            n_components=400,      # 🔧 à ajuster selon la taille du corpus\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])),\n",
    "        (\"nav\", Pipeline([\n",
    "            (\"sel_nav\", SelectCols([\"navigateur\"])),\n",
    "            (\"to_2d\", FunctionTransformer(lambda df: df.values, validate=False)),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "        ]))\n",
    "    ], n_jobs=-1)),\n",
    "    (\"clf\", LinearSVC(C=4.0))\n",
    "])\n",
    "\n",
    "# ========= Features agrégées + XGBoost =========\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"agg\", Pipeline([\n",
    "        (\"sel_raw\", SelectCols([\"seq_raw\"])),\n",
    "        (\"stats\", SessionStats()),\n",
    "        (\"to_df\", FunctionTransformer(lambda a: pd.DataFrame(\n",
    "            a, columns=[\n",
    "                \"n_actions\", \"n_twins\", \"n_unique_ctrl\", \"n_unique_actions\",\n",
    "                \"ratio_unique\", \"ratio_twins\", \"ratio_actions\",\n",
    "                \"n_conf\", \"n_chain\", \"twin_span\", \"twin_gap_mean\"\n",
    "            ]), validate=False))\n",
    "    ])),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Le méta-modèle (final)\n",
    "#meta = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
    "meta = LinearSVC(C=4.0)\n",
    "# Combinaison des deux pipelines\n",
    "reduced_stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"svm\", pipe_svm),\n",
    "        (\"xgb\", pipe_xgb)\n",
    "    ],\n",
    "    final_estimator=meta,\n",
    "    stack_method=\"auto\", \n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "reduced_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[[\"seq_raw\", \"seq_txt\", \"navigateur\"]].copy()\n",
    "y = train[\"util\"].astype(str)\n",
    "\n",
    "# Encode labels pour XGBoost\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "f1_macro = make_scorer(f1_score, average=\"macro\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Cross-validation ---\n",
    "reduced_scores = cross_val_score(reduced_stack, X, y_enc, cv=cv, scoring=f1_macro, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd116b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"marco-F1-score moyen après l'application de TruncatedSVD pour simplifier les données : {np.mean(reduced_scores):.4f} ± {np.std(reduced_scores):.4f}\")\n",
    "print(\"Scores par fold :\", np.round(reduced_scores, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b564bd2",
   "metadata": {},
   "source": [
    "Après l’application du **TruncatedSVD** pour réduire la dimension des vecteurs TF-IDF de **20000 composantes** à **400 composantes**, le modèle atteint un **F1-score macro moyen de 0.9572**.  \n",
    "Ce score reste très proche de celui obtenu avant la réduction (≈ 0.962), ce qui montre que **la simplification du modèle n’entraîne qu’une légère perte de performance** tout en offrant l'avantage important de la **réduction significative de la taille des features** et donc un gain en efficacité pour la mémoire,  \n",
    "\n",
    "En résumé, le **TruncatedSVD** permet de conserver l’essentiel de l’information textuelle tout en rendant le pipeline plus efficace et plus robuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1fd18",
   "metadata": {},
   "source": [
    "# 6. Soumission Kaggle\n",
    "Même si la réduction de dimension par **TruncatedSVD** permet de simplifier la représentation textuelle tout en maintenant un bon niveau de performance (**F1-macro = 0.946**),  \n",
    "nous avons choisi d’utiliser, pour la **soumission finale sur Kaggle**, le **modèle complet sans réduction de dimension**, qui atteint un **F1-macro de 0.958**.\n",
    "\n",
    "Ce choix s’explique principalement par le fait que, malgré la réduction du nombre de dimensions, **l’application du SVD augmente la consommation mémoire**, car elle nécessite de stocker la matrice dense projetée.  \n",
    "De plus, la légère perte d’information observée lors de la projection dans un espace réduit peut pénaliser la détection de classes spécifiques et surtout les classes rares.\n",
    "\n",
    "Ainsi, pour la compétition Kaggle, où l’objectif est d’obtenir la **meilleure performance possible**, nous conservons le **modèle le plus complet**, qui exploite toute la richesse des vecteurs TF-IDF d’origine.  \n",
    "La version réduite avec SVD reste néanmoins intéressante pour des analyses exploratoires ou des contextes à ressources limitées.\n",
    "\n",
    "⚠️ **Important :** pour la soumission finale, nous n’allons pas utiliser le modèle entraîné sur 80 % des données et validé sur 20 %,  \n",
    "mais plutôt **réentraîner le pipeline de stacking sur l’ensemble du jeu de données**, en conservant les mêmes hyperparamètres.  \n",
    "Cette approche permet de **mieux exploiter toutes les données disponibles** pour améliorer la généralisation du modèle  \n",
    "et **potentiellement obtenir un score plus élevé sur le leaderboard Kaggle**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55571ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.fit(X, y_enc)\n",
    "\n",
    "test_df = test[[\"seq_raw\",\"seq_txt\",\"navigateur\"]].copy()\n",
    "test_pred = stack.predict(test_df)\n",
    "test_pred = le.inverse_transform(test_pred)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"RowId\": np.arange(len(test_pred)) + 1,\n",
    "    \"prediction\": test_pred\n",
    "})\n",
    "\n",
    "sub.to_csv(\"../submissions/submission_v3.csv\", index=False)\n",
    "print(\"Saved submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEsVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
